{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPyM+g+cYyBkygwyeT/Yl0O"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# download the dataset"],"metadata":{"id":"rOyOfY4BNPiq"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6qw6SIe1L3pR","executionInfo":{"status":"ok","timestamp":1742787139749,"user_tz":-420,"elapsed":2521,"user":{"displayName":"Tuyên Giáp","userId":"05290698715950561497"}},"outputId":"f4508274-4f10-4be9-d014-7baeadd2a229"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting opendatasets\n","  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from opendatasets) (4.67.1)\n","Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (from opendatasets) (1.7.4.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from opendatasets) (8.1.8)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (6.2.0)\n","Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2025.1.31)\n","Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (3.4.1)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (3.10)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (5.29.3)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.8.2)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (8.0.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.32.3)\n","Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (75.1.0)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.17.0)\n","Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.3)\n","Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.3.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (0.5.1)\n","Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n","Installing collected packages: opendatasets\n","Successfully installed opendatasets-0.1.22\n"]}],"source":["!pip install opendatasets"]},{"cell_type":"code","source":["import opendatasets as od\n","od.download(\"https://www.kaggle.com/datasets/aladdinpersson/flickr8kimagescaptions\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A6sPAjcHNV3c","executionInfo":{"status":"ok","timestamp":1742787226247,"user_tz":-420,"elapsed":79919,"user":{"displayName":"Tuyên Giáp","userId":"05290698715950561497"}},"outputId":"4e1ef931-05ab-4dea-de57-b43fca7e0241"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n","Your Kaggle username: titus-tanashi\n","Your Kaggle Key: ··········\n","Dataset URL: https://www.kaggle.com/datasets/aladdinpersson/flickr8kimagescaptions\n"]}]},{"cell_type":"markdown","source":["#take random 10% of dataset"],"metadata":{"id":"gXONBotZP5LC"}},{"cell_type":"code","source":["captions_path = \"/content/flickr8kimagescaptions/flickr8k/captions.txt\"\n","image_folder = \"/content/flickr8kimagescaptions/flickr8k/images\""],"metadata":{"id":"jUwuRKjuNdNY","executionInfo":{"status":"ok","timestamp":1742788582669,"user_tz":-420,"elapsed":5,"user":{"displayName":"Tuyên Giáp","userId":"05290698715950561497"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["from collections import defaultdict\n","import random\n","image_captions = defaultdict(list)\n","\n","with open(captions_path, \"r\", encoding=\"utf-8\") as f:\n","    next(f)  # Skip header if needed\n","    for line in f:\n","        line = line.strip()\n","        if not line:\n","            continue  # Skip empty lines\n","        parts = line.split(\",\", 1)  # Split at first comma to avoid errors\n","        if len(parts) < 2:\n","            print(f\"Skipping malformed line: {line}\")  # Debugging info\n","            continue  # Skip invalid lines\n","        img_name, caption = parts\n","        image_captions[img_name].append(caption.lower().split())\n","sample_images = random.sample(list(image_captions.keys()), int(len(image_captions) * 0.1))"],"metadata":{"id":"mDMh2Bd8OeGi","executionInfo":{"status":"ok","timestamp":1742788584771,"user_tz":-420,"elapsed":190,"user":{"displayName":"Tuyên Giáp","userId":"05290698715950561497"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["#call the model"],"metadata":{"id":"ilLBHbwmQ1lz"}},{"cell_type":"code","source":["from transformers import AutoModelForVision2Seq, AutoProcessor\n","import torch\n","model_path = \"/content/model\"  # Path to your trained model\n","processor_path = \"/content/processor\"  # Path to your processor\n","\n","model = AutoModelForVision2Seq.from_pretrained(model_path)\n","processor = AutoProcessor.from_pretrained(processor_path)\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ycs5qpDxQ6R_","executionInfo":{"status":"ok","timestamp":1742788439399,"user_tz":-420,"elapsed":2579,"user":{"displayName":"Tuyên Giáp","userId":"05290698715950561497"}},"outputId":"75e4caba-0abc-4de6-810b-a148281280af"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BlipForConditionalGeneration(\n","  (vision_model): BlipVisionModel(\n","    (embeddings): BlipVisionEmbeddings(\n","      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","    )\n","    (encoder): BlipEncoder(\n","      (layers): ModuleList(\n","        (0-11): 12 x BlipEncoderLayer(\n","          (self_attn): BlipAttention(\n","            (dropout): Dropout(p=0.0, inplace=False)\n","            (qkv): lora.Linear(\n","              (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n","              (lora_dropout): ModuleDict(\n","                (default): Dropout(p=0.05, inplace=False)\n","              )\n","              (lora_A): ModuleDict(\n","                (default): Linear(in_features=768, out_features=16, bias=False)\n","              )\n","              (lora_B): ModuleDict(\n","                (default): Linear(in_features=16, out_features=2304, bias=False)\n","              )\n","              (lora_embedding_A): ParameterDict()\n","              (lora_embedding_B): ParameterDict()\n","              (lora_magnitude_vector): ModuleDict()\n","            )\n","            (projection): lora.Linear(\n","              (base_layer): Linear(in_features=768, out_features=768, bias=True)\n","              (lora_dropout): ModuleDict(\n","                (default): Dropout(p=0.05, inplace=False)\n","              )\n","              (lora_A): ModuleDict(\n","                (default): Linear(in_features=768, out_features=16, bias=False)\n","              )\n","              (lora_B): ModuleDict(\n","                (default): Linear(in_features=16, out_features=768, bias=False)\n","              )\n","              (lora_embedding_A): ParameterDict()\n","              (lora_embedding_B): ParameterDict()\n","              (lora_magnitude_vector): ModuleDict()\n","            )\n","          )\n","          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): BlipMLP(\n","            (activation_fn): GELUActivation()\n","            (fc1): lora.Linear(\n","              (base_layer): Linear(in_features=768, out_features=3072, bias=True)\n","              (lora_dropout): ModuleDict(\n","                (default): Dropout(p=0.05, inplace=False)\n","              )\n","              (lora_A): ModuleDict(\n","                (default): Linear(in_features=768, out_features=16, bias=False)\n","              )\n","              (lora_B): ModuleDict(\n","                (default): Linear(in_features=16, out_features=3072, bias=False)\n","              )\n","              (lora_embedding_A): ParameterDict()\n","              (lora_embedding_B): ParameterDict()\n","              (lora_magnitude_vector): ModuleDict()\n","            )\n","            (fc2): lora.Linear(\n","              (base_layer): Linear(in_features=3072, out_features=768, bias=True)\n","              (lora_dropout): ModuleDict(\n","                (default): Dropout(p=0.05, inplace=False)\n","              )\n","              (lora_A): ModuleDict(\n","                (default): Linear(in_features=3072, out_features=16, bias=False)\n","              )\n","              (lora_B): ModuleDict(\n","                (default): Linear(in_features=16, out_features=768, bias=False)\n","              )\n","              (lora_embedding_A): ParameterDict()\n","              (lora_embedding_B): ParameterDict()\n","              (lora_magnitude_vector): ModuleDict()\n","            )\n","          )\n","          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (text_decoder): BlipTextLMHeadModel(\n","    (bert): BlipTextModel(\n","      (embeddings): BlipTextEmbeddings(\n","        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (encoder): BlipTextEncoder(\n","        (layer): ModuleList(\n","          (0-11): 12 x BlipTextLayer(\n","            (attention): BlipTextAttention(\n","              (self): BlipTextSelfAttention(\n","                (query): lora.Linear(\n","                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=768, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=768, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (key): lora.Linear(\n","                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=768, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=768, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (value): lora.Linear(\n","                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=768, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=768, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (output): BlipTextSelfOutput(\n","                (dense): lora.Linear(\n","                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=768, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=768, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","            )\n","            (crossattention): BlipTextAttention(\n","              (self): BlipTextSelfAttention(\n","                (query): lora.Linear(\n","                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=768, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=768, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (key): lora.Linear(\n","                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=768, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=768, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (value): lora.Linear(\n","                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=768, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=768, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (output): BlipTextSelfOutput(\n","                (dense): lora.Linear(\n","                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=768, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=768, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","            )\n","            (intermediate): BlipTextIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BlipTextOutput(\n","              (dense): lora.Linear(\n","                (base_layer): Linear(in_features=3072, out_features=768, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=3072, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=768, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (cls): BlipTextOnlyMLMHead(\n","      (predictions): BlipTextLMPredictionHead(\n","        (transform): BlipTextPredictionHeadTransform(\n","          (dense): Linear(in_features=768, out_features=768, bias=True)\n","          (transform_act_fn): GELUActivation()\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n","      )\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["#evaluate model"],"metadata":{"id":"wBjTofRuSY2v"}},{"cell_type":"code","source":["pip install rouge_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tc8ZA_vaSq_t","executionInfo":{"status":"ok","timestamp":1742788552931,"user_tz":-420,"elapsed":6314,"user":{"displayName":"Tuyên Giáp","userId":"05290698715950561497"}},"outputId":"73da8b93-51bf-41ae-e1ac-b96a69927e5f"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n","Building wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=c373960d78c59716f174b9ec41741a4b878c0e8632b3a07d0da25a99931d740a\n","  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n","Successfully built rouge_score\n","Installing collected packages: rouge_score\n","Successfully installed rouge_score-0.1.2\n"]}]},{"cell_type":"code","source":["from nltk.translate.bleu_score import sentence_bleu\n","from rouge_score import rouge_scorer\n","from PIL import Image\n","import os"],"metadata":{"id":"LM-GOCy5SkNH","executionInfo":{"status":"ok","timestamp":1742788555063,"user_tz":-420,"elapsed":5,"user":{"displayName":"Tuyên Giáp","userId":"05290698715950561497"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["bleu_scores = {1: [], 2: [], 3: [], 4: []}\n","rouge_l_scores = []"],"metadata":{"id":"sO3T7gxJSTzJ","executionInfo":{"status":"ok","timestamp":1742788558526,"user_tz":-420,"elapsed":46,"user":{"displayName":"Tuyên Giáp","userId":"05290698715950561497"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["for img_name in sample_images:\n","    img_path = os.path.join(image_folder, img_name)\n","    reference_captions = image_captions[img_name]\n","\n","    # Load and preprocess image\n","    image = Image.open(img_path).convert(\"RGB\")\n","    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n","\n","    # Generate Caption\n","    with torch.no_grad():\n","        output_ids = model.generate(**inputs)\n","    generated_caption = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n","    generated_tokens = generated_caption.lower().split()\n","\n","    # Compute BLEU Scores\n","    for n in range(1, 5):\n","        weights = [1/n] * n + [0] * (4 - n)  # Adjust weights\n","        bleu_scores[n].append(sentence_bleu(reference_captions, generated_tokens, weights=weights))\n","\n","    # Compute ROUGE-L Score\n","    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    score = scorer.score(\" \".join(reference_captions[0]), generated_caption)\n","    rouge_l_scores.append(score[\"rougeL\"].fmeasure)\n","\n","# Compute Average Scores\n","avg_bleu = {f\"BLEU-{k}\": sum(v)/len(v) for k, v in bleu_scores.items()}\n","avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n","\n","# Print Results\n","print(\"Evaluation Results on 10% Flickr8k:\")\n","print(avg_bleu)\n","print(f\"ROUGE-L: {avg_rouge_l:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ksHcSSPSgWG","executionInfo":{"status":"ok","timestamp":1742789006465,"user_tz":-420,"elapsed":417243,"user":{"displayName":"Tuyên Giáp","userId":"05290698715950561497"}},"outputId":"ab9fd11c-af84-457b-dbad-e1bc68a4d091"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n","The hypothesis contains 0 counts of 3-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n","The hypothesis contains 0 counts of 2-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"]},{"output_type":"stream","name":"stdout","text":["Evaluation Results on 10% Flickr8k:\n","{'BLEU-1': 0.6576936566836769, 'BLEU-2': 0.489008008637832, 'BLEU-3': 0.3327776390740428, 'BLEU-4': 0.20073460277328123}\n","ROUGE-L: 0.4479\n"]}]}]}